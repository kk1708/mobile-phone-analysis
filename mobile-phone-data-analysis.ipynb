{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "encouraging-costa",
   "metadata": {},
   "source": [
    "# Data Analysis on Mobile Phone Features\n",
    "With technology becoming cheaper and more accessible, multiple new faces emerged into the mobile phone market - each offering a unique set of features at affordable prices.\n",
    "\n",
    "Given a price range, today's customers have too wide a variety of selections to choose from. The options are endless, and choosing the **best** product becomes unnecessarily complicated.\n",
    "\n",
    "To solve this problem, I present this analysis, which will help potential customers simplify their selection process by **classifying the best phones** into different categories as follows:\n",
    "- **The Daily Driver** -> For people who want reliable phones for daily use\n",
    "- **The Cameraman** -> For casual photographers and videographers who use mobile phones as part of their workflow\n",
    "- **The Performer** -> For gamers and others who require performance over everything else\n",
    "- **The Monk** -> For people who want something simple, yet robust, without all the *smart* stuff\n",
    "\n",
    "Based on their requirements, customers can now narrow their options down to the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-module",
   "metadata": {},
   "source": [
    "## Explaining the data\n",
    "The dataset used to perform this analysis contains data of 1000 different mobile phones - in a similar price range - along with all their features.\n",
    "\n",
    "The dataset used in this analysis [can be found here](https://www.kaggle.com/iabhishekofficial/mobile-price-classification)\n",
    "\n",
    "To further understand the data and perform exploratory analysis, we have to examine dataset. To do this, we use the `pandas` library, which is part of the Python programming language. The `pandas` library contains useful functions for representing, analysing and visualising data.\n",
    "\n",
    "For more information on the `pandas` library [refer this link](https://pandas.pydata.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-answer",
   "metadata": {},
   "source": [
    "We use the built-in `import` Python command to import the `pandas` library into our project. We use the `as` command to create an alias for `pandas`, which can be used to call the methods of `pandas`\n",
    "\n",
    "We use the command `pd.read_csv()` to take the data present in `dataset.csv` and store it in a variable called `dataframe`\n",
    "\n",
    "This variable is used to represent the entire dataset and to perform further anaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-indication",
   "metadata": {},
   "source": [
    "By calling `dataframe` we get access to our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-recommendation",
   "metadata": {},
   "source": [
    "Now that we've represented the data, it is time to explain it. The dataset contains 1000 rows and 21 columns. The rows represent the different mobile phones while the columns represent the different features offered by each.\n",
    "\n",
    "As there are too many columns, we cannot see the entire list. Hence, we need to print out all the different columns available in our dataset so that we may look into all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-lotus",
   "metadata": {},
   "source": [
    "By using `.columns` we get a list of all the columns available in the dataset. Using this list, we can identify the features of each phone.\n",
    "\n",
    "### The Features of each phone (column headers)\n",
    "\n",
    "- `battery_power` -> The battery capacity in mAH (higher values indicate better battery life)\n",
    "- `blue`-> Whether the phone supports Bluetooth (The value `1` indicates that it supports Bluetooth)\n",
    "- `clock_speed` -> How fast the processor computes tasks (higher value indicates better performance)\n",
    "- `dual_sim` -> Whether the phone supports dual sim (The value `1` indicates that it supports dual sim)\n",
    "- `fc` -> The pixels of the front camera (higher value indicates better quality selfies)\n",
    "- `four_g` -> Whether the phone supports 4G (The value `1` indicates that it supports 4G)\n",
    "- `int_memory` -> The internal memory of the phone (higher value indicates more storage)\n",
    "- `m-dep` -> Indicates the depth of the phone\n",
    "- `mobile_wt` -> Indicates the weight of the phone\n",
    "- `n_cores` -> Represents the number of cores (higher value indicates better efficiency)\n",
    "- `pc` -> The pixels of the primary camera (higher value indicates better photos and videos)\n",
    "- `px_height` -> The pixel height of the phone\n",
    "- `px_width` -> The pixel width of the phone\n",
    "- `ram` -> The amount of temporary memory available (higher value indicates better multi tasking)\n",
    "- `sc_h` -> The screen height\n",
    "- `sc_w` -> The screen width\n",
    "- `talk_time` -> The longest time the battery will last after a full charge (higher value indicates more battery life)\n",
    "- `three_g` -> Whether the phone supports 3G (The value `1` indicates that it supports 3G)\n",
    "- `touch_screen` -> Whether the phone has a touch screen (The value `1` indicates that it has a touch screen)\n",
    "- `wifi` -> Whether the phone supports Wi-Fi (The value `1` indicates that it supports Wi-Fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-roulette",
   "metadata": {},
   "source": [
    "## Data Cleanup\n",
    "\n",
    "Now that we've understood the data, it is time to do a cleanup. The cleanup process involves:\n",
    "1. Checking for missing or inconsistent data\n",
    "2. Removing/modifying irrelevant columns\n",
    "3. Removing elements that do not satisfy certain conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-radical",
   "metadata": {},
   "source": [
    "### 1) Checking for missing or inconsistent data\n",
    "\n",
    "In any given dataset, there is a probability of missing or inconsistent data\n",
    "\n",
    "**Missing data** pertains to any data that should have be present in the dataset, but is not. Depending on the dataset, there are several solutions to this problem:\n",
    "- Remove the entire row containing the missing data\n",
    "- Replace the missing data with the average value along the entire dataset\n",
    "- Replace the missing data with the average value of it's successor and predecessor\n",
    "- Replace the missing data with a random value in a particular range\n",
    "\n",
    "**Inconsistent data** occurs when similar data is kept in different formats along different files. Care should be taken in maintaining data integrity when dealing with multiple files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-portal",
   "metadata": {},
   "source": [
    "To detect missing values in a dataset we use `.isna()`, which returns `True` if data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-consolidation",
   "metadata": {},
   "source": [
    "From the operation performed, it _seems like_ there are no missing values since `dataframe.isna()` returned `False`\n",
    "\n",
    "But this output is a condensed version of the actual output. This result doesn't guarantee the presence of all values.\n",
    "\n",
    "To dig deeper, we use the `any()` operation, which returns `True` along an axis if an element is present. This operation is performed twice in conjuction to check along both axis simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.isna().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-diamond",
   "metadata": {},
   "source": [
    "**Since the output of this operation is `False`, we can conclude that there are no missing values in the given dataset.** If instead, the above operation returned `True`, then we would have to cleanup the dataset to fix the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-birmingham",
   "metadata": {},
   "source": [
    "**And since all data is stored in a single file, it is consistent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-yemen",
   "metadata": {},
   "source": [
    "### 2) Removing/modifying irrelevant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-mobile",
   "metadata": {},
   "source": [
    "Given a dataset, it is important to look into every attribute to check whether it provides useful information. Chances are that every database contains irrelevant attributes or columns that do not give us any insights. Removing or/and modifying these attributes will result in a more compact and cleaner dataset.\n",
    "\n",
    "So once again, we look at the columns of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-nursery",
   "metadata": {},
   "source": [
    "Observing these attributes, we find the `three_g` attribute to be obsolete. 3G cannot meet the high speed demands of today. Hence, we remove the `three_g` attribute because it is irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframe['three_g']\n",
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-action",
   "metadata": {},
   "source": [
    "By using the `del` command and by specifying the column name, the `three_g` attribute is now removed. The command `dataframe.columns` shows the successful deletion of the attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-configuration",
   "metadata": {},
   "source": [
    "Similarly, the attribute `m_dep` that stands for mobile depth is also removed. Mobile depth is a factor that is almost never considered by a potential customer and thus, is not required in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframe['m_dep']\n",
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-bangladesh",
   "metadata": {},
   "source": [
    "We've successfully removed `m_dep` as indicated above by the `dataframe.columns` command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-gathering",
   "metadata": {},
   "source": [
    "#### Merging Attributes together\n",
    "Observing the dataset once again, the `px_height` and `px_width` attributes can be merged together to create a new attribute `resolution`, which gives us the display resolution of the mobile phone. This information is more useful than the separate height and width\n",
    "\n",
    "**To create the new `resolution` attribute,** we use the values in `px_height` and `px_width` along with the **screen size of the mobile phone** to find the PPI ([Pixels Per Inch](https://en.wikipedia.org/wiki/Pixel_density)), which is a common metric used for resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-toddler",
   "metadata": {},
   "source": [
    "`PPI = diagonal length (pixels) / diagonal length (inches)`\n",
    "\n",
    "Diagonal lengths (both in pixels and inches) can be found by taking the [Euclidean Distance](https://en.wikipedia.org/wiki/Euclidean_distance) of the height and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# creating four numpy arrays containing the pixel height, pixel width, screen height and screen width respectively\n",
    "pixelHeight = dataframe['px_height'].to_numpy()\n",
    "pixelWidth = dataframe['px_width'].to_numpy()\n",
    "screenHeight = dataframe['sc_h'].to_numpy()\n",
    "screenWidth = dataframe['sc_w'].to_numpy()\n",
    "\n",
    "# calculating the diagonal values in pixels and inches\n",
    "diagPix = pow(pixelHeight*pixelHeight + pixelWidth*pixelWidth,1/2)\n",
    "diagInch = pow(screenHeight*screenHeight + screenWidth*screenWidth,1/2)\n",
    "\n",
    "# calculating PPI using the formula given above\n",
    "ppi = diagPix/diagInch\n",
    "\n",
    "# adding the PPI values to a the attribute 'resolution'\n",
    "dataframe['resolution'] = np.floor(ppi).tolist()\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-russell",
   "metadata": {},
   "source": [
    "Finding PPI (Pixels Per Inch) requires certain calculations as per the formula. These calculations are hard to perform using `pandas`. Fortunately, there is a Python library called `numpy` that is specifically used for numerical computations and which also integrates seamlessly with `pandas`\n",
    "\n",
    "[Click here for more information on numpy](https://numpy.org/)\n",
    "\n",
    "To use the `numpy` library, we use the command `import numpy as np`, where `np` is the alias used to access the different `numpy` methods.\n",
    "\n",
    "Four `numpy` arrays are created whose values are equal to the attributes:\n",
    "- `px_height`\n",
    "- `px_width`\n",
    "- `sc_h`\n",
    "- and `sc_w` respectively\n",
    "\n",
    "Using these arrays, we can perform the Euclidean operation to find the diagonal lengths in both pixels (`diagPix`) and inches (`diagInch`). These diagonal lengths are also `numpy` arrays\n",
    "\n",
    "Once these values are calculated, PPI is simply the quotient of `diagPix` over `diagInch`. A `numpy` floor operation - `np.floor()` - eliminates the decimals before adding the values to the new attribute `resolution`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-essay",
   "metadata": {},
   "source": [
    "The next step is removing the attributes `px_height` and `px_width`, which are now redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframe['px_height'], dataframe['px_width']\n",
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-worse",
   "metadata": {},
   "source": [
    "### 3) Removing elements that do not satisfy certain conditions\n",
    "\n",
    "It is now time to look at the elements of the dataset - namely the rows. Our dataset contains 1000 elements - mobile phones along with their features - but not all of them are satisfactory. That is, there are some elements (mobile phones) in the dataset that do not satisfy the basic requirements of a customer. \n",
    "\n",
    "For example: **WiFi, Mobile Data and Bluetooth.**\n",
    "\n",
    "In the modern world, these three features are expected to be in every phone. Customers do not even consider them because it is a given that they are present in every phone. **Hence, those phones not having these features are removed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[['blue','four_g','wifi']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-queensland",
   "metadata": {},
   "source": [
    "By examining the `blue`, `four_g` and `wifi` attributes, we find that their presence or lack of it is represented by the values `1` and `0` respectively. Now, we use the `if` statement to prune out the elements that do not contain all three features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-venice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
